{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import dill\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of a TicTacToe environment, call reset() to initialize the game and list the available agents (players):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['player_1', 'player_2']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = tictactoe_v3.env()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps to understand exactly how the PettingZoo \"mechanics\" work.\n",
    "\n",
    "Below we use the agent_selection method to show exactly when the active agent switches between the players:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'player_1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.agent_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'player_1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#env.step()\n",
    "# env acts by updating the observation and \n",
    "# switches to the next player\n",
    "env.agent_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now player 2 can act\n",
    "env.step(1)\n",
    "env.agent_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, directly after an agent takes an action using env.step(), the game moves on to the environent which \"acts\" by updating the board position, and after that the other player can act."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Using action masks to choose from available actions\n",
    "\n",
    "The TicTacToe PettingZoo environment uses so-called \"action masks\" to filter out actions that are invalid or not available given the current state of the environment. The action mask is part of the observation output from last().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, done, info = env.last()\n",
    "\n",
    "observation['action_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mask tells us that for the current agent, actions 0 and 1 are not available. Our policy() function needs this information for action selection.\n",
    "\n",
    "If we choose an illegal action the environment throws an error message and terminates the current game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# player 1\n",
    "env.step(0)\n",
    "# player 2 attempts same move\n",
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done is True , we can let the agent play action None. This allows the agents to keep on stepping until all rewards are received by all agents.\n",
    "\n",
    "For example this game where Player 1 plays the winning move:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.step(0)\n",
    "env.step(3)\n",
    "env.step(1)\n",
    "env.step(4)\n",
    "env.step(2)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now the player is Player 2, that receives its (negative) reward for losing the game. Note that it cannot play any legal moves anymore because the game has ended, but we need to call step() with action None to move back to Player 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, done, info = env.last()\n",
    "\n",
    "print(done)\n",
    "print(reward)\n",
    "\n",
    "env.step(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Player 2 is removed from the list of available agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env.agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now player 1 can collect its reward for winning the game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, done, info = env.last()\n",
    "\n",
    "print(done)\n",
    "print(reward)\n",
    "\n",
    "env.step(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no active agents anymore, need to call env.reset() to start a new game\n",
    "env.agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 Random play\n",
    "When two players who play completely randomly play Tic-Tac-Toe, the first player wins 58.49% of the time, the second player wins 28.81% of the time, and the game is a draw 12.70% of the time.\n",
    "\n",
    "Code up a function that has both players play a random policy for 10.000 games. Store the outcomes of the games (W/D/L) for both Players. Check your work by comparing with the percentages above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this as starting point\n",
    "\n",
    "def policy(observation, agent):\n",
    "    action = random.choice(np.flatnonzero(observation['action_mask']))\n",
    "    return action\n",
    "\n",
    "env.reset()\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, done, info = env.last()\n",
    "    action = policy(observation, agent) if not done else None\n",
    "    env.step(action)\n",
    "    env.render() # this visualizes a single game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashing the board position to a key for use in a dictionary\n",
    "\n",
    "In the TicTacToe environment, observations of agents consist of a complete description of the board position. An observation of the board is a 3D array and looks like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "observation, reward, done, info = env.last()\n",
    "\n",
    "\n",
    "observation['observation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the properly rendered board position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Q-learning, we need the environment to store Q-values for each unique board position. A convenient way to create unique identifiers for all board positions is to use a hash-function.\n",
    "\n",
    "We encountered this concept at the beginning of the course:\n",
    "\n",
    "        Hash functions are used to transform a large amount of data (such as a complete board position aka game state) into a single number. The special thing about hash functions is that every board position is transformed into a unique number, i.e. there are no two board positions that are transformed to the same unique number. This allows us to use this to label / identify each board position, and use this as an identifier to store information about that board position.\n",
    "\n",
    "Example code (first convert observation to string, then hash):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = hash(str(observation['observation']))\n",
    "\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Update: I discovered that in Python 3, the hash() function is, by design, not reproducible between python sessions! This makes it unsuitable for our purpose, since we want to learn an optimal policy for each state, and save that policy (the Q-table) to disk for later use. This later use will consist of things like testing the policy's performance, or as an AI player to play against ourselves.\n",
    "\n",
    "To have reproducible hashing we can use hashlib, a Python library containing various hashing algorithms. I chose the MD5 algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def encode_state(observation):\n",
    "    # encode observation as bytes           \n",
    "    obs_bytes = str(observation).encode('utf-8')\n",
    "    # create md5 hash\n",
    "    m = hashlib.md5(obs_bytes)\n",
    "    # return hash as hex digest\n",
    "    state = m.hexdigest()\n",
    "    return(state)\n",
    "\n",
    "encode_state(observation['observation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To make self-play (An single agent that plays against itself) easy to implement, in PettingZoo the observation contains information about which player is making the observation. This information is encoded in the observation by flipping the board position player index order (aka \"inverting the channels\").\n",
    "\n",
    "Take for example the observation of the board state after Player 1 made a first move:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = tictactoe_v3.env()\n",
    "env.reset()\n",
    "env.step(4)\n",
    "env.observe('player_1')['observation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare this with how Player 2 observes the same board position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observe('player_2')['observation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In practice, this is only an issue if both players see the same board position, which only occurs at the end of a game, when the players collect their rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = tictactoe_v3.env()\n",
    "env.reset()\n",
    "env.step(0)\n",
    "env.step(6)\n",
    "env.step(1)\n",
    "env.step(5)\n",
    "env.step(2)\n",
    "\n",
    "env.observe(\"player_1\")['observation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(None)\n",
    "env.observe(\"player_2\")['observation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid double counting of end-game board positions, I used this trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = encode_state(env.render(mode = 'ansi'))\n",
    "\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 Hashing and dictionaries for the Q-table\n",
    "For this exercise, adapt your code from Exercise 1 to add a defaultdict dictionary that contains the value 0 for each board position (identified using encode_state() ) the agents encounter.\n",
    "\n",
    "Run your code for 20.000 games with the agents playing a random policy to find out how many distinct states Tic-Tac-Toe contains. The dictionary should max out at 5478 different states.\n",
    "\n",
    "You can use the code provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "env.reset()\n",
    "\n",
    "Q = defaultdict(lambda: np.zeros(nA)) \n",
    "\n",
    "# reminder about how default dict works\n",
    "\n",
    "Q['32433'] = 0\n",
    "Q['-5323'] = 0\n",
    "Q['2397887'] = 0\n",
    "\n",
    "Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('zoo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88a93e89b8a11d2f445313f46f93cc3120b7248de9ac4e9c249ecb557e7cf9b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
